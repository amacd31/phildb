{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hydrological Reference Stations (HRS) Example\n",
    "==\n",
    "This notebook demonstrates loading time series data into a PhilDB instance.\n",
    "It shows an example of adding timeseries, measurand, and source to the database before loading the time series data as time series instances.\n",
    "\n",
    "All instructions in this notebook are relative to the examples/hrs/ directory of the phildb project.\n",
    "\n",
    "Before running the code the HRS data set needs to be downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "r = requests.get('http://www.bom.gov.au/water/hrs/content/config/site_config.json')\n",
    "hrs_metadata = r.json()\n",
    "\n",
    "for station in x['stations']['features']:\n",
    "    station_id = station['properties']['AWRC_ID']\n",
    "    filename = '{0}_daily_ts.csv'.format(station_id)\n",
    "\n",
    "    url = \"http://www.bom.gov.au/water/hrs/content/data/{0}/{1}\".format(station_id, filename)\n",
    "\n",
    "    csv_response = request.get(url)\n",
    "    with open(os.path.join('data', filename), 'w') as f:\n",
    "        f.write(csv_response.text)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data we can do some standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new PhilDB database\n",
    "--\n",
    "\n",
    "The next snippet shows how to create a PhilDB database using the create method. Alternate to the below code the commandline phil-create method could be used (e.g `phil-create hrs_db`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from phildb.create import create\n",
    "create('hrs_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing a PhilDB instance\n",
    "--\n",
    "\n",
    "Once a PhilDB database has been created it can be accessed using the PhilDB database class. Which after being imported can be used to create a database instance (which is stored in the 'db' variable here) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from phildb.database import PhilDB\n",
    "db = PhilDB('hrs_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise source and measurand attributes\n",
    "--\n",
    "\n",
    "Now that we have created and connected to the 'hrs_db' we can initialise source and measurand attributes for identifying HRS time series instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.add_measurand('Q', 'STREAMFLOW', 'Streamflow')\n",
    "db.add_source('BOM_HRS', 'Bureau of Meteorology; Hydrological Reference Stations dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a frequency variable `freq` to 'D' to indicate daily data and a hrs_header_len variable so that the 18 lines of header in the CSV can be handled later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq = 'D'\n",
    "hrs_header_len = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function for reading HRS data into a pandas DataFrame (this is to simplify the for loop doing the actual data loading):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_hrs_series(filename):\n",
    "    with open(filename) as datafile:\n",
    "        header=[next(datafile) for x in range(hrs_header_len)]\n",
    "        header = ''.join(header)\n",
    "        df = pd.read_csv(filename, parse_dates=True, index_col='Date', skiprows=hrs_header_len)\n",
    "\n",
    "        return header, df['Q']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of csv files from the hrs_data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafiles = [ f for f in os.listdir('hrs_data') if f.endswith('_daily_ts.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file:  102101A_daily_ts.csv ...\n",
      "Using station ID:  102101A ...\n",
      "Processing file:  104001A_daily_ts.csv ...\n",
      "Using station ID:  104001A ...\n",
      "Processing file:  105101A_daily_ts.csv ...\n",
      "Using station ID:  105101A ...\n",
      "Processing file:  105102A_daily_ts.csv ...\n",
      "Using station ID:  105102A ...\n",
      "Processing file:  105105A_daily_ts.csv ...\n",
      "Using station ID:  105105A ...\n",
      "Processing file:  107001B_daily_ts.csv ...\n",
      "Using station ID:  107001B ...\n",
      "Processing file:  108002A_daily_ts.csv ...\n",
      "Using station ID:  108002A ...\n",
      "Processing file:  108003A_daily_ts.csv ...\n",
      "Using station ID:  108003A ...\n",
      "Processing file:  112002A_daily_ts.csv ...\n",
      "Using station ID:  112002A ...\n",
      "Processing file:  112102A_daily_ts.csv ...\n",
      "Using station ID:  112102A ...\n",
      "Processing file:  113004A_daily_ts.csv ...\n",
      "Using station ID:  113004A ...\n",
      "Processing file:  116006B_daily_ts.csv ...\n",
      "Using station ID:  116006B ...\n",
      "Processing file:  116010A_daily_ts.csv ...\n",
      "Using station ID:  116010A ...\n",
      "Processing file:  116011A_daily_ts.csv ...\n",
      "Using station ID:  116011A ...\n",
      "Processing file:  116012A_daily_ts.csv ...\n",
      "Using station ID:  116012A ...\n",
      "Processing file:  116013A_daily_ts.csv ...\n",
      "Using station ID:  116013A ...\n",
      "Processing file:  116014A_daily_ts.csv ...\n",
      "Using station ID:  116014A ...\n",
      "Processing file:  116015A_daily_ts.csv ...\n",
      "Using station ID:  116015A ...\n",
      "Processing file:  121001A_daily_ts.csv ...\n",
      "Using station ID:  121001A ...\n",
      "Processing file:  122004A_daily_ts.csv ...\n",
      "Using station ID:  122004A ...\n",
      "Processing file:  126003A_daily_ts.csv ...\n",
      "Using station ID:  126003A ...\n",
      "Processing file:  136202D_daily_ts.csv ...\n",
      "Using station ID:  136202D ...\n",
      "Processing file:  136203A_daily_ts.csv ...\n",
      "Using station ID:  136203A ...\n",
      "Processing file:  136208A_daily_ts.csv ...\n",
      "Using station ID:  136208A ...\n",
      "Processing file:  137101A_daily_ts.csv ...\n",
      "Using station ID:  137101A ...\n",
      "Processing file:  137201A_daily_ts.csv ...\n",
      "Using station ID:  137201A ...\n",
      "Processing file:  138004B_daily_ts.csv ...\n",
      "Using station ID:  138004B ...\n",
      "Processing file:  138009A_daily_ts.csv ...\n",
      "Using station ID:  138009A ...\n",
      "Processing file:  138010A_daily_ts.csv ...\n",
      "Using station ID:  138010A ...\n",
      "Processing file:  138113A_daily_ts.csv ...\n",
      "Using station ID:  138113A ...\n",
      "Processing file:  143009A_daily_ts.csv ...\n",
      "Using station ID:  143009A ...\n",
      "Processing file:  143110A_daily_ts.csv ...\n",
      "Using station ID:  143110A ...\n",
      "Processing file:  143303A_daily_ts.csv ...\n",
      "Using station ID:  143303A ...\n",
      "Processing file:  145010A_daily_ts.csv ...\n",
      "Using station ID:  145010A ...\n",
      "Processing file:  145011A_daily_ts.csv ...\n",
      "Using station ID:  145011A ...\n",
      "Processing file:  145018A_daily_ts.csv ...\n",
      "Using station ID:  145018A ...\n",
      "Processing file:  145101D_daily_ts.csv ...\n",
      "Using station ID:  145101D ...\n",
      "Processing file:  145107A_daily_ts.csv ...\n",
      "Using station ID:  145107A ...\n",
      "Processing file:  146010A_daily_ts.csv ...\n",
      "Using station ID:  146010A ...\n",
      "Processing file:  146012A_daily_ts.csv ...\n",
      "Using station ID:  146012A ...\n",
      "Processing file:  146014A_daily_ts.csv ...\n",
      "Using station ID:  146014A ...\n",
      "Processing file:  146095A_daily_ts.csv ...\n",
      "Using station ID:  146095A ...\n",
      "Processing file:  204034_daily_ts.csv ...\n",
      "Using station ID:  204034 ...\n",
      "Processing file:  206014_daily_ts.csv ...\n",
      "Using station ID:  206014 ...\n",
      "Processing file:  206018_daily_ts.csv ...\n",
      "Using station ID:  206018 ...\n",
      "Processing file:  208007_daily_ts.csv ...\n",
      "Using station ID:  208007 ...\n",
      "Processing file:  208009_daily_ts.csv ...\n",
      "Using station ID:  208009 ...\n",
      "Processing file:  210006_daily_ts.csv ...\n",
      "Using station ID:  210006 ...\n",
      "Processing file:  210011_daily_ts.csv ...\n",
      "Using station ID:  210011 ...\n",
      "Processing file:  211008_daily_ts.csv ...\n",
      "Using station ID:  211008 ...\n",
      "Processing file:  212209_daily_ts.csv ...\n",
      "Using station ID:  212209 ...\n",
      "Processing file:  212260_daily_ts.csv ...\n",
      "Using station ID:  212260 ...\n",
      "Processing file:  215002_daily_ts.csv ...\n",
      "Using station ID:  215002 ...\n",
      "Processing file:  215004_daily_ts.csv ...\n",
      "Using station ID:  215004 ...\n",
      "Processing file:  216002_daily_ts.csv ...\n",
      "Using station ID:  216002 ...\n",
      "Processing file:  216004_daily_ts.csv ...\n",
      "Using station ID:  216004 ...\n",
      "Processing file:  218001_daily_ts.csv ...\n",
      "Using station ID:  218001 ...\n",
      "Processing file:  219001_daily_ts.csv ...\n",
      "Using station ID:  219001 ...\n",
      "Processing file:  221207_daily_ts.csv ...\n",
      "Using station ID:  221207 ...\n",
      "Processing file:  221210_daily_ts.csv ...\n",
      "Using station ID:  221210 ...\n",
      "Processing file:  222206_daily_ts.csv ...\n",
      "Using station ID:  222206 ...\n",
      "Processing file:  222213_daily_ts.csv ...\n",
      "Using station ID:  222213 ...\n",
      "Processing file:  223202_daily_ts.csv ...\n",
      "Using station ID:  223202 ...\n",
      "Processing file:  224206_daily_ts.csv ...\n",
      "Using station ID:  224206 ...\n",
      "Processing file:  224213A_daily_ts.csv ...\n",
      "Using station ID:  224213A ...\n",
      "Processing file:  224214A_daily_ts.csv ...\n",
      "Using station ID:  224214A ...\n",
      "Processing file:  225020A_daily_ts.csv ...\n",
      "Using station ID:  225020A ...\n",
      "Processing file:  225110A_daily_ts.csv ...\n",
      "Using station ID:  225110A ...\n",
      "Processing file:  225219_daily_ts.csv ...\n",
      "Using station ID:  225219 ...\n",
      "Processing file:  226220_daily_ts.csv ...\n",
      "Using station ID:  226220 ...\n",
      "Processing file:  226222_daily_ts.csv ...\n",
      "Using station ID:  226222 ...\n",
      "Processing file:  226407_daily_ts.csv ...\n",
      "Using station ID:  226407 ...\n",
      "Processing file:  227225A_daily_ts.csv ...\n",
      "Using station ID:  227225A ...\n",
      "Processing file:  227226_daily_ts.csv ...\n",
      "Using station ID:  227226 ...\n",
      "Processing file:  227227_daily_ts.csv ...\n",
      "Using station ID:  227227 ...\n",
      "Processing file:  229650A_daily_ts.csv ...\n",
      "Using station ID:  229650A ...\n",
      "Processing file:  229661A_daily_ts.csv ...\n",
      "Using station ID:  229661A ...\n",
      "Processing file:  230210_daily_ts.csv ...\n",
      "Using station ID:  230210 ...\n",
      "Processing file:  231213_daily_ts.csv ...\n",
      "Using station ID:  231213 ...\n",
      "Processing file:  235205_daily_ts.csv ...\n",
      "Using station ID:  235205 ...\n",
      "Processing file:  236213_daily_ts.csv ...\n",
      "Using station ID:  236213 ...\n",
      "Processing file:  238208_daily_ts.csv ...\n",
      "Using station ID:  238208 ...\n",
      "Processing file:  302214_daily_ts.csv ...\n",
      "Using station ID:  302214 ...\n",
      "Processing file:  304497_daily_ts.csv ...\n",
      "Using station ID:  304497 ...\n",
      "Processing file:  304499_daily_ts.csv ...\n",
      "Using station ID:  304499 ...\n",
      "Processing file:  305202_daily_ts.csv ...\n",
      "Using station ID:  305202 ...\n",
      "Processing file:  307473_daily_ts.csv ...\n",
      "Using station ID:  307473 ...\n",
      "Processing file:  308145_daily_ts.csv ...\n",
      "Using station ID:  308145 ...\n",
      "Processing file:  308799_daily_ts.csv ...\n",
      "Using station ID:  308799 ...\n",
      "Processing file:  312061_daily_ts.csv ...\n",
      "Using station ID:  312061 ...\n",
      "Processing file:  314207_daily_ts.csv ...\n",
      "Using station ID:  314207 ...\n",
      "Processing file:  314213_daily_ts.csv ...\n",
      "Using station ID:  314213 ...\n",
      "Processing file:  315450_daily_ts.csv ...\n",
      "Using station ID:  315450 ...\n",
      "Processing file:  318076_daily_ts.csv ...\n",
      "Using station ID:  318076 ...\n",
      "Processing file:  401009_daily_ts.csv ...\n",
      "Using station ID:  401009 ...\n",
      "Processing file:  401012_daily_ts.csv ...\n",
      "Using station ID:  401012 ...\n",
      "Processing file:  401015_daily_ts.csv ...\n",
      "Using station ID:  401015 ...\n",
      "Processing file:  401203_daily_ts.csv ...\n",
      "Using station ID:  401203 ...\n",
      "Processing file:  401208_daily_ts.csv ...\n",
      "Using station ID:  401208 ...\n",
      "Processing file:  401210_daily_ts.csv ...\n",
      "Using station ID:  401210 ...\n",
      "Processing file:  401212_daily_ts.csv ...\n",
      "Using station ID:  401212 ...\n",
      "Processing file:  401216_daily_ts.csv ...\n",
      "Using station ID:  401216 ...\n",
      "Processing file:  401217_daily_ts.csv ...\n",
      "Using station ID:  401217 ...\n",
      "Processing file:  402204_daily_ts.csv ...\n",
      "Using station ID:  402204 ...\n",
      "Processing file:  402206_daily_ts.csv ...\n",
      "Using station ID:  402206 ...\n",
      "Processing file:  402213_daily_ts.csv ...\n",
      "Using station ID:  402213 ...\n",
      "Processing file:  402217_daily_ts.csv ...\n",
      "Using station ID:  402217 ...\n",
      "Processing file:  403209A_daily_ts.csv ...\n",
      "Using station ID:  403209A ...\n",
      "Processing file:  403213A_daily_ts.csv ...\n",
      "Using station ID:  403213A ...\n",
      "Processing file:  403214_daily_ts.csv ...\n",
      "Using station ID:  403214 ...\n",
      "Processing file:  403217_daily_ts.csv ...\n",
      "Using station ID:  403217 ...\n",
      "Processing file:  403221_daily_ts.csv ...\n",
      "Using station ID:  403221 ...\n",
      "Processing file:  403222_daily_ts.csv ...\n",
      "Using station ID:  403222 ...\n",
      "Processing file:  403226_daily_ts.csv ...\n",
      "Using station ID:  403226 ...\n",
      "Processing file:  403232_daily_ts.csv ...\n",
      "Using station ID:  403232 ...\n",
      "Processing file:  404207_daily_ts.csv ...\n",
      "Using station ID:  404207 ...\n",
      "Processing file:  405205_daily_ts.csv ...\n",
      "Using station ID:  405205 ...\n",
      "Processing file:  405209_daily_ts.csv ...\n",
      "Using station ID:  405209 ...\n",
      "Processing file:  405215_daily_ts.csv ...\n",
      "Using station ID:  405215 ...\n",
      "Processing file:  405217_daily_ts.csv ...\n",
      "Using station ID:  405217 ...\n",
      "Processing file:  405218_daily_ts.csv ...\n",
      "Using station ID:  405218 ...\n",
      "Processing file:  405219_daily_ts.csv ...\n",
      "Using station ID:  405219 ...\n",
      "Processing file:  405226_daily_ts.csv ...\n",
      "Using station ID:  405226 ...\n",
      "Processing file:  405227_daily_ts.csv ...\n",
      "Using station ID:  405227 ...\n",
      "Processing file:  405238_daily_ts.csv ...\n",
      "Using station ID:  405238 ...\n",
      "Processing file:  405245_daily_ts.csv ...\n",
      "Using station ID:  405245 ...\n",
      "Processing file:  405248_daily_ts.csv ...\n",
      "Using station ID:  405248 ...\n",
      "Processing file:  405251_daily_ts.csv ...\n",
      "Using station ID:  405251 ...\n",
      "Processing file:  405263_daily_ts.csv ...\n",
      "Using station ID:  405263 ...\n",
      "Processing file:  405264_daily_ts.csv ...\n",
      "Using station ID:  405264 ...\n",
      "Processing file:  405274_daily_ts.csv ...\n",
      "Using station ID:  405274 ...\n",
      "Processing file:  406208_daily_ts.csv ...\n",
      "Using station ID:  406208 ...\n",
      "Processing file:  406213_daily_ts.csv ...\n",
      "Using station ID:  406213 ...\n",
      "Processing file:  406214_daily_ts.csv ...\n",
      "Using station ID:  406214 ...\n",
      "Processing file:  406224_daily_ts.csv ...\n",
      "Using station ID:  406224 ...\n",
      "Processing file:  407214_daily_ts.csv ...\n",
      "Using station ID:  407214 ...\n",
      "Processing file:  407215_daily_ts.csv ...\n",
      "Using station ID:  407215 ...\n",
      "Processing file:  407220_daily_ts.csv ...\n",
      "Using station ID:  407220 ...\n",
      "Processing file:  407230_daily_ts.csv ...\n",
      "Using station ID:  407230 ...\n",
      "Processing file:  407253_daily_ts.csv ...\n",
      "Using station ID:  407253 ...\n",
      "Processing file:  408200_daily_ts.csv ...\n",
      "Using station ID:  408200 ...\n",
      "Processing file:  408202_daily_ts.csv ...\n",
      "Using station ID:  408202 ...\n",
      "Processing file:  410057_daily_ts.csv ...\n",
      "Using station ID:  410057 ...\n",
      "Processing file:  410061_daily_ts.csv ...\n",
      "Using station ID:  410061 ...\n",
      "Processing file:  410705_daily_ts.csv ...\n",
      "Using station ID:  410705 ...\n",
      "Processing file:  410730_daily_ts.csv ...\n",
      "Using station ID:  410730 ...\n",
      "Processing file:  410731_daily_ts.csv ...\n",
      "Using station ID:  410731 ...\n",
      "Processing file:  410734_daily_ts.csv ...\n",
      "Using station ID:  410734 ...\n",
      "Processing file:  410761_daily_ts.csv ...\n",
      "Using station ID:  410761 ...\n",
      "Processing file:  412028_daily_ts.csv ...\n",
      "Using station ID:  412028 ...\n",
      "Processing file:  412050_daily_ts.csv ...\n",
      "Using station ID:  412050 ...\n",
      "Processing file:  412066_daily_ts.csv ...\n",
      "Using station ID:  412066 ...\n",
      "Processing file:  415207_daily_ts.csv ...\n",
      "Using station ID:  415207 ...\n",
      "Processing file:  415226_daily_ts.csv ...\n",
      "Using station ID:  415226 ...\n",
      "Processing file:  415237_daily_ts.csv ...\n",
      "Using station ID:  415237 ...\n",
      "Processing file:  416003_daily_ts.csv ...\n",
      "Using station ID:  416003 ...\n",
      "Processing file:  416008_daily_ts.csv ...\n",
      "Using station ID:  416008 ...\n",
      "Processing file:  418005_daily_ts.csv ...\n",
      "Using station ID:  418005 ...\n",
      "Processing file:  418014_daily_ts.csv ...\n",
      "Using station ID:  418014 ...\n",
      "Processing file:  419005_daily_ts.csv ...\n",
      "Using station ID:  419005 ...\n",
      "Processing file:  422202B_daily_ts.csv ...\n",
      "Using station ID:  422202B ...\n",
      "Processing file:  422306A_daily_ts.csv ...\n",
      "Using station ID:  422306A ...\n",
      "Processing file:  422313B_daily_ts.csv ...\n",
      "Using station ID:  422313B ...\n",
      "Processing file:  422319B_daily_ts.csv ...\n",
      "Using station ID:  422319B ...\n",
      "Processing file:  422321B_daily_ts.csv ...\n",
      "Using station ID:  422321B ...\n",
      "Processing file:  422334A_daily_ts.csv ...\n",
      "Using station ID:  422334A ...\n",
      "Processing file:  422394A_daily_ts.csv ...\n",
      "Using station ID:  422394A ...\n",
      "Processing file:  424002_daily_ts.csv ...\n",
      "Using station ID:  424002 ...\n",
      "Processing file:  424201A_daily_ts.csv ...\n",
      "Using station ID:  424201A ...\n",
      "Processing file:  604053_daily_ts.csv ...\n",
      "Using station ID:  604053 ...\n",
      "Processing file:  606001_daily_ts.csv ...\n",
      "Using station ID:  606001 ...\n",
      "Processing file:  606002_daily_ts.csv ...\n",
      "Using station ID:  606002 ...\n",
      "Processing file:  606185_daily_ts.csv ...\n",
      "Using station ID:  606185 ...\n",
      "Processing file:  607155_daily_ts.csv ...\n",
      "Using station ID:  607155 ...\n",
      "Processing file:  608002_daily_ts.csv ...\n",
      "Using station ID:  608002 ...\n",
      "Processing file:  610008_daily_ts.csv ...\n",
      "Using station ID:  610008 ...\n",
      "Processing file:  613002_daily_ts.csv ...\n",
      "Using station ID:  613002 ...\n",
      "Processing file:  613146_daily_ts.csv ...\n",
      "Using station ID:  613146 ...\n",
      "Processing file:  614044_daily_ts.csv ...\n",
      "Using station ID:  614044 ...\n",
      "Processing file:  616002_daily_ts.csv ...\n",
      "Using station ID:  616002 ...\n",
      "Processing file:  616013_daily_ts.csv ...\n",
      "Using station ID:  616013 ...\n",
      "Processing file:  616065_daily_ts.csv ...\n",
      "Using station ID:  616065 ...\n",
      "Processing file:  803003_daily_ts.csv ...\n",
      "Using station ID:  803003 ...\n",
      "Processing file:  804001_daily_ts.csv ...\n",
      "Using station ID:  804001 ...\n",
      "Processing file:  809310_daily_ts.csv ...\n",
      "Using station ID:  809310 ...\n",
      "Processing file:  912101A_daily_ts.csv ...\n",
      "Using station ID:  912101A ...\n",
      "Processing file:  912105A_daily_ts.csv ...\n",
      "Using station ID:  912105A ...\n",
      "Processing file:  915011A_daily_ts.csv ...\n",
      "Using station ID:  915011A ...\n",
      "Processing file:  917107A_daily_ts.csv ...\n",
      "Using station ID:  917107A ...\n",
      "Processing file:  919003A_daily_ts.csv ...\n",
      "Using station ID:  919003A ...\n",
      "Processing file:  919201A_daily_ts.csv ...\n",
      "Using station ID:  919201A ...\n",
      "Processing file:  919309A_daily_ts.csv ...\n",
      "Using station ID:  919309A ...\n",
      "Processing file:  922101B_daily_ts.csv ...\n",
      "Using station ID:  922101B ...\n",
      "Processing file:  925001A_daily_ts.csv ...\n",
      "Using station ID:  925001A ...\n",
      "Processing file:  926002A_daily_ts.csv ...\n",
      "Using station ID:  926002A ...\n",
      "Processing file:  A0020101_daily_ts.csv ...\n",
      "Using station ID:  A0020101 ...\n",
      "Processing file:  A0030501_daily_ts.csv ...\n",
      "Using station ID:  A0030501 ...\n",
      "Processing file:  A2390519_daily_ts.csv ...\n",
      "Using station ID:  A2390519 ...\n",
      "Processing file:  A2390523_daily_ts.csv ...\n",
      "Using station ID:  A2390523 ...\n",
      "Processing file:  A2390531_daily_ts.csv ...\n",
      "Using station ID:  A2390531 ...\n",
      "Processing file:  A5030502_daily_ts.csv ...\n",
      "Using station ID:  A5030502 ...\n",
      "Processing file:  A5040517_daily_ts.csv ...\n",
      "Using station ID:  A5040517 ...\n",
      "Processing file:  A5040523_daily_ts.csv ...\n",
      "Using station ID:  A5040523 ...\n",
      "Processing file:  A5050517_daily_ts.csv ...\n",
      "Using station ID:  A5050517 ...\n",
      "Processing file:  A5130501_daily_ts.csv ...\n",
      "Using station ID:  A5130501 ...\n",
      "Processing file:  G0010005_daily_ts.csv ...\n",
      "Using station ID:  G0010005 ...\n",
      "Processing file:  G0050115_daily_ts.csv ...\n",
      "Using station ID:  G0050115 ...\n",
      "Processing file:  G0060005_daily_ts.csv ...\n",
      "Using station ID:  G0060005 ...\n",
      "Processing file:  G8110004_daily_ts.csv ...\n",
      "Using station ID:  G8110004 ...\n",
      "Processing file:  G8110016_daily_ts.csv ...\n",
      "Using station ID:  G8110016 ...\n",
      "Processing file:  G8140001_daily_ts.csv ...\n",
      "Using station ID:  G8140001 ...\n",
      "Processing file:  G8140040_daily_ts.csv ...\n",
      "Using station ID:  G8140040 ...\n",
      "Processing file:  G8140161_daily_ts.csv ...\n",
      "Using station ID:  G8140161 ...\n",
      "Processing file:  G8150018_daily_ts.csv ...\n",
      "Using station ID:  G8150018 ...\n",
      "Processing file:  G8170002_daily_ts.csv ...\n",
      "Using station ID:  G8170002 ...\n",
      "Processing file:  G8190001_daily_ts.csv ...\n",
      "Using station ID:  G8190001 ...\n",
      "Processing file:  G8200045_daily_ts.csv ...\n",
      "Using station ID:  G8200045 ...\n",
      "Processing file:  G8210010_daily_ts.csv ...\n",
      "Using station ID:  G8210010 ...\n",
      "Processing file:  G9030124_daily_ts.csv ...\n",
      "Using station ID:  G9030124 ...\n",
      "Processing file:  G9030250_daily_ts.csv ...\n",
      "Using station ID:  G9030250 ...\n",
      "Processing file:  G9070142_daily_ts.csv ...\n",
      "Using station ID:  G9070142 ...\n"
     ]
    }
   ],
   "source": [
    "for filename in datafiles:\n",
    "    print(\"Processing file: \", filename, '...')\n",
    "    station_id = filename.split('_')[0]\n",
    "    print(\"Using station ID: \", station_id, '...')\n",
    "\n",
    "    header, streamflow = read_hrs_series(os.path.join('hrs_data', filename))\n",
    "    db.add_timeseries(station_id)\n",
    "    db.add_timeseries_instance(station_id, freq, header, measurand = 'Q', source= 'BOM_HRS')\n",
    "    db.write(station_id, freq, streamflow, measurand = 'Q', source = 'BOM_HRS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Open the newly created PhilDB for interactive exploration:\n",
    "\n",
    "    phil hrs_db\n",
    "\n",
    "An example script, autocorr.py, shows how access to a PhilDB instance\n",
    "can be automated to perform analysis on the data. This script for example\n",
    "iterates over all of the available streamflow timeseries instances for the\n",
    "BOM_HRS dataset and invokes the pandas autocorr function. Stations with\n",
    "auto-correlation results greater than or equal to 0.95 are then printed.\n",
    "\n",
    "The script can be run with:\n",
    "\n",
    "    python autocorr.py hrs_db\n",
    "\n",
    "Such analysis could be performed from the interactive phil shell as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
